{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "chess_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir6bRzL-hWS3"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u1YYdC4fZDF"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk_QX5Fj262O"
      },
      "source": [
        "run_in_colab = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw5izra19Bqg"
      },
      "source": [
        "if run_in_colab:\n",
        "  !pip install transformers\n",
        "  !pip install wandb\n",
        "  !pip install git+https://github.com/google-research/bleurt.git\n",
        "  !pip install datasets\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  \n",
        "  !git clone https://github.com/nofarmordehai/Learn-Chess-Commentary.git 'chess'\n",
        "  CODE_DIR = 'chess'\n",
        "  os.chdir(f'./{CODE_DIR}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJIGy7y9cZHa"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import time\n",
        "from Models.GPT2 import GPT2\n",
        "from Models.BERT import BERT\n",
        "from Dataset.MovesDataset import MovesDataset\n",
        "from Configs.train_config import config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goRDizAYcZHk"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j93CnZ1d9F_Y"
      },
      "source": [
        "if run_in_colab:\n",
        "  BASE_PATH = '/content/drive/MyDrive/NLP/'\n",
        "else:\n",
        "  BASE_PATH = '/home/joberant/nlp_fall_2021/nofarm/chess/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VI0s_4NmSbN"
      },
      "source": [
        "games_data_path = BASE_PATH + 'Data/NEW_attack/games_data'\n",
        "saved_models_path = BASE_PATH + 'Models/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVorMAYKgPKM"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ7N_8oETXAk"
      },
      "source": [
        "if config['Model'] == 'gpt2':\n",
        "  gpt2 = GPT2()\n",
        "  model = gpt2.model.train()\n",
        "  tokenizer = gpt2.tokenizer\n",
        "  max_length = 768\n",
        "  eof = '<|endoftext|>'\n",
        "\n",
        "elif config['Model'] == 'bert-base':\n",
        "  bert = BERT()\n",
        "  model = bert.model.train()\n",
        "  tokenizer = bert.tokenizer\n",
        "  max_length = 512\n",
        "  eof = 'endoftext'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7AMjxd-h_jW"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh7s2375iGK0"
      },
      "source": [
        "# The right way to do train-test split:\n",
        "# dataset = MovesDataset([f'{games_data_path}{i+1}.p' for i in range(config['NUMER_OF_DATA_DIRS']-1)], tokenizer, max_length=max_length) \n",
        "# train_size = int(config['train_precentege'] * len(dataset))\n",
        "# test_size = len(dataset) - train_size\n",
        "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6ztgCQwU1Dx"
      },
      "source": [
        "# but we calculate our metrics afterwards with the saved models, so we keep the last pickle for testing:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_EbPuQSiDOs"
      },
      "source": [
        "dataset = MovesDataset([f'{games_data_path}{i+1}.p' for i in range(config['NUMER_OF_DATA_DIRS']-1)], tokenizer, max_length=max_length) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNPlpmzLiJ5M"
      },
      "source": [
        "train_dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNS1749vUbh4"
      },
      "source": [
        "test_dataset = MovesDataset([f'{games_data_path}{i}.p' for i in [config['NUMER_OF_DATA_DIRS']] ], tokenizer, max_length=max_length) \n",
        "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHbKE9y8zUlU"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NPueVq5i7d2"
      },
      "source": [
        "Validation Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VAR464QjVN7"
      },
      "source": [
        "run = wandb.init(project=\"LmChess\", config={'batch size': config['batch_size'], 'lr': config['lr'], 'epochs': config['epochs']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRQuMVr4wDKW"
      },
      "source": [
        "validation_proccessed_data, validation_attn_masks, validation_labels = next(iter(test_dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYOII4EU73_W"
      },
      "source": [
        "validation_input_encodings = []\n",
        "for i in range(config['batch_size']):\n",
        "  textual_validation_data = tokenizer.decode(token_ids = validation_proccessed_data[i], skip_special_tokens=False).split('<comment>')\n",
        "\n",
        "  validation_target_text = textual_validation_data[1].split(eof)[0]\n",
        "  validation_input_text = textual_validation_data[0] \n",
        "\n",
        "  wandb.log({f\"validation_target_text {i}\": wandb.Html(f'<p>{validation_target_text}</p>')})\n",
        "  wandb.log({f\"validation_input_text {i}\": wandb.Html(f'<p>{validation_input_text}</p>')})\n",
        "\n",
        "  comment_idx = list(validation_proccessed_data[i]).index(dataset.comment_encoding) + 1\n",
        "  validation_input_encoding = validation_proccessed_data[i][:comment_idx].unsqueeze(0).cuda()\n",
        "  #validation_input_encoding  = tokenizer.encode(validation_input_text, return_tensors=\"pt\").cuda()\n",
        "  \n",
        "  validation_input_encodings.append(validation_input_encoding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDUJKUoyE4BX"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r2OySwlmIlc"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr= config['lr'])\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=5000, num_training_steps=-1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_QD1UtPRPoa"
      },
      "source": [
        "loss = 0\n",
        "pad_token_id = tokenizer('[PAD]')['input_ids'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNcrRkmMmQcL"
      },
      "source": [
        "epochs = config['epochs']\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(total=len(dataset) / 2) as pbar:\n",
        "        for idx,entry in enumerate(train_dataloader):\n",
        "\n",
        "            if idx % 2000 == 0 and idx != 0:\n",
        "              for i in range(config['batch_size']):\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(validation_input_encodings[i], num_beams=2, no_repeat_ngram_size=2, max_length=max_length+1, pad_token_id=pad_token_id)\n",
        "                    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                wandb.log({f\"output_text {i}\": wandb.Html(f'<p>{output_text}</p>')})\n",
        "            \n",
        "            if idx % 50000 == 0:\n",
        "              torch.save(model.state_dict(), f'{saved_models_path}{idx}_{time.time()}_{int(loss)}.bin')\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            inputs = entry[0].cuda()\n",
        "            attn_masks = entry[1].cuda()\n",
        "            labels = entry[2].cuda()\n",
        "            outputs = model(inputs, labels=labels, attention_mask = attn_masks)\n",
        "\n",
        "            loss = outputs['loss']\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
        "            pbar.update(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVqX0F8ClYG3"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwnIbQcWdL29"
      },
      "source": [
        "every t iterations calculate evaluation metrics for the current model and save the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhhbfW1cIcmj"
      },
      "source": [
        "from Evaluation.Metrics import perplexity, bleurt, bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZq8YhOdZJUX"
      },
      "source": [
        "from Utils import get_targets_and_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-uo5LT8IcF2"
      },
      "source": [
        "test_perplexity = perplexity(model, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tQK57G1Ib-F"
      },
      "source": [
        "target_texts, output_texts = get_targets_and_outputs(model, test_dataset, dataset.comment_encoding, pad_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUmPJBVEcrl0"
      },
      "source": [
        "test_bleurt = bleurt(target_texts, output_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2vcBxDDjdJ"
      },
      "source": [
        "test_bleu = bleu(target_texts, output_texts)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}